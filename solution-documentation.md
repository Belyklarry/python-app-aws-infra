# Solution Documentation
## Why choose Amazon Web Services (AWS)
AWS is the largest and most adopted cloud platform that offers over 200 fully featured services from its global data centers. AWS has significantly more services and features than any other cloud provider. Amongst its services is the Amazon Elastic Kubernetes Services (EKS). EKS is a managed Kubernetes service to run Kubernetes in the AWS cloud. Amazon EKS automatically manages the availability and scalability of the Kubernetes Control plane nodes which are responsible for the scheduling of containers, managing application availability, storing cluster data, and other key tasks. It allows you to take full advantage of the other AWS services such as the compute, storage, security, and networking services. 

## Container Orchestration with Kubernetes
Kubernetes is an open-source solution for automating the deployment, scaling, and management of containerized applications. It brings together all the containers that make up an application and bundles them up into logical units for easy management and discovery, hence its choice for our solution. AWS EKS is our Kubernetes solution choice because of the above-mentioned AWS EKS advantages. 
## Infrastructure as code (IaC) with Terraform
Deploying our EKS clusters manually in AWS using the AWS console is time-consuming and lacks uniformity when we need to spin up other similar clusters. This is where Terraform comes in. Terraform is an infrastructure as code tool that lets you define your entire cloud (e.g. AWS) infrastructure in human-readable configuration files that can be versioned, reused, and shared. This provides a solution for consistent workflows in the provisioning of cloud infrastructure. In our solution, we’ll deploy a Kubernetes (EKS) cluster with Terraform. With Terraform, we are able to keep track of our cloud infrastructure in a state file, which acts as the source of truth for our environment. In order to quickly deploy our application, we’ll host our state files locally since we’re not yet deploying to production. However, when we are now expanding the application to production, we will migrate our state to the AWS S3 bucket or Terraform Cloud (Where the state can be versioned) because a remote state is considered best practice. The remote state uses state locking to prevent concurrent runs of Terraform against the same state. This improves team coordination when working with Terraform. 

## EKS Blueprints for Terraform
Instead of deploying our EKS cluster using the default EKS module offered by Terraform, we’ll make use of the EKS blueprints for Terraform. EKS Blueprints enable us to compose complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. With EKS Blueprints, you describe the configuration of the desired state of your EKS environment, such as the control plane, worker nodes, and Kubernetes add-ons as an IaC blueprint. Once the blueprint has been configured, you can use it to deploy consistent environments across multiple AWS accounts and Regions using continuous deployment automation, for example, GitLab CI/CD.
# Blueprint for our Development Environment
In our ```clj-app-infra``` repo, the terraform configuration files (or manifests) are the ones with the ```.tf``` extension. Now let us go into what each of the manifests does.

## The providers.tf manifest
Providers in Terraform are plugins that allow users to manage external APIs. Provider plugins, like the AWS provider, act as the translation layer that allows Terraform to communicate with many different cloud providers, databases, and services. Before we can execute any terraform CLI commands, we need to tell Terraform the versions of the providers we are using, as well as the version of Terraform to use.

## The data.tf manifest
Here we are making use of data sources that allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, e.g. AWS.

## The outputs.tf manifest
With output values, the information about our infrastructure becomes available on the command line and we can expose information for other Terraform configurations to use. Output values are like return values in programming languages. In our solution, we will output the VPC and related subnets, as well as the command to add the newly created cluster to our Kubernetes ```~/.kube/config``` configuration file, which enables access to our cluster.
## The locals.tf manifest
A local value assigns a name to an expression, so you can use the name multiple times within a module without having to repeat the expression. 
Here we also added the ArgoCD application configuration for the add-ons and workloads repositories.
## The main.tf manifest
Here we use the Terraform AWS VPC Module to provision an Amazon Virtual Private Cloud and its subnets. A VPC enables you to launch AWS resources into a virtual network that you’ve defined. This virtual network closely resembles a traditional network that you’d operate in your own data center, with the benefits of using the scalable infrastructure of AWS. Our EKS cluster will be deployed in this VPC. 
On top of our main.tf file, we add some provider-specific configuration to allow Kubernetes, Helm, and Kubectl to access our EKS cluster. 
We then add the ```eks_blueprint``` for the Terraform core module and include the EKS managed node group. The managed node group will automate the provisioning and lifecycle management of nodes for the EKS cluster.
We’ll pin the main EKS Blueprint module to v4.9.0 which corresponds to the GitHub repo release Tag. It is good practice to lock in all your modules to a given tried and tested version.
Another important thing to add to our eks_blueprint module is the Platform Team which will manage the EKS cluster provisioning. Under the hood, this will create a dedicated role that will allow you to manage the cluster as an administrator. We can also define existing users/roles that will be allowed to assume this role via the users parameter where we can provide a list of IAM ARNs. The new role is configured in the EKS Configmap to allow authentication.
We also onboarded the application team and called it clj-microservices.  This will also create a dedicated role that’ll allow you to manage the clj-microservices team authentication in EKS. The created role will also be configured in the EKS Configmap. The clj-microservices team being created is in fact a Kubernetes namespace associated with RBAC and quotas. We are also using the ```manifest_dir``` directory to install specific Kubernetes manifests in the namespace at creation time. We also created a default ```limit range``` that will inject default resources limits to our pods if they are not defined. The limit range is in the ```./kubernetes/clj-microservices``` directory.
It is best practice to not create Kubernetes objects with kubectl directly but to rely on continuous deployment tools, for example, GitOps with ArgoCD.
We have created a Github repo for our [workloads](https://github.com/Belyklarry/eks-blueprints-workloads.git). In this repo, we defined the development environment as well as the clj-microservices team. Our Clojure microservices application has been containerized and we have separated the microservices into Kubernetes Deployments. We then stacked the deployments together using the Helm format under ```teams/clj-microservices/dev/```. This enables us to uniformly package our application into a single stack. Helm is a tool that streamlines installing and managing Kubernetes applications. Think of it as apt/yum/homebrew for Kubernetes. Helm renders your templates and communicates with the Kubernetes API and hence installing your application. The Dockerfiles for the microservice applications can be accessed from this [repo](https://github.com/Belyklarry/clojure-microservices-app.git) in their respective directories. Our Kubernetes manifest files are in  ```teams/clj-microservices/dev/templates/```. Whenever we make changes to the Kubernetes manifests, ArgoCD syncs the changes and deploys them onto our development environment.
We configured the eks-blueprints-add-ons [repository](https://github.com/aws-samples/eks-blueprints-add-ons.git) to manage the EKS add-ons for our cluster, using ArgoCD. Deploying the Kubernetes add-ons with GitOps has several advantages, like their state will always be synchronized with the workloads repo thanks to the ArgoCD controller.
Another thing we added in our ```main.tf``` file is ``` the kubernetes_addons``` module. To indicate that ArgoCD should be responsible for managing cluster add-ons, we enable the ```argocd_manage_add_ons`` property. When this flag is set, the framework will still provision all AWS resources necessary to support add-on functionalities (IAM Roles, IAM Policies, dependent resources..), but it'll not apply Helm charts directly via the Terraform Helm provider, and lets ArgoCD do it. We also specified a custom set to configure ArgoCD to expose ArgoCD UI on an aws load balancer. This will configure the ArgoCD add-on, and allow it to deploy additional Kubernetes add-ons using GitOps. To access the ArgoCD UI, run the following command:
```console
export ARGOCD_SERVER=`kubectl get svc argo-cd-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'`
echo "https://$ARGOCD_SERVER"
```
Copy and paste the URL onto your browser to access the UI. Since ArgoCD UI makes use of a self-signed certificate, you’ll need to accept the security exception in your browser to access it. The user name is ``admin`` and the password is obtained from the following command:
```console
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
```
Still in the workloads repo, the ```envs/dev/values.yaml``` configures the source [repoURL](https://github.com/Belyklarry/eks-blueprints-workloads.git) that’ll be used by the ArgoCD Application definition. The ```envs/dev/templates/clj-microservices-app.yaml``` is an ArgoCD Application definition that specifies which repo and directory to use.
To activate our workloads, we add it under argocd_applications configuration in our ```main.tf``` file. We can view information about our application in the ArgoCD UI. ArgoCD will show all the Kubernetes components created, e.g., pods, services, ingress, logs, etc.
# Future Work
## Extending the terraform scripts via GitLab CI/CD
GitLab CI/CD is a tool for software development using continuous methodologies. In our case, it automatically takes care of the terraform commands when deploying our infrastructure. When we commit changes to the main branch of our infrastructure repository, the GitLab pipeline is triggered, and it executes the tasks defined in the ```.gitlab-ci.yml``` file. The ```.gitlab-ci.yml``` file is where we’ll define all the stages terraform needs to go through to deploy our infrastructure to AWS. We can set it up in a way that the pipeline must successfully complete before the changes made can be merged into the main branch. This way we automate error detection and it avoids the merging of malformed terraform scripts. To grant GitLab access to our AWS account, we provide our AWS Access and Secret keys in the GitLab environmental variables. We have included an example of the pipeline in the ```clj-app-infra``` repository.
## Adding Environments
To add more environments to our EKS cluster, we’ll make use of our workloads repository. Under ```envs/``` we can add our environments as directories. Within these environment directories, we organize our files and directories in the Helm format. In the ```templates/``` directory, we define the team and environment in an ArgoCD ```Application``` kind manifest. This will create a dedicated namespace for our different environments. We can also define the quotas for the environment under the application team and label the team with its respective environment in our ```main.tf``` file.
## Hardening into a Production System
To harden our infrastructure into a production system, later on, we’ll make full use of the EKS blueprints. The EKS Blueprints will allow us to easily add a range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, Fluent Bit, Grafana, and more. We’ll also easily onboard new teams and team members onto our EKS cluster easily in the same manner in which we added the clj-microservices team. Onboarding of the different team's workloads will be done in the workloads repository and allow ArgoCD to handle the Continous Delivery. 
